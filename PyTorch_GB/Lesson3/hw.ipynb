{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попрактикуемся с тем, что изучили\n",
    "\n",
    "Будем практиковаться на датасете:\n",
    "https://www.kaggle.com/c/avito-demand-prediction\n",
    "\n",
    "Ваша задача:\n",
    "1. Создать Dataset для загрузки данных (sklearn.datasets.fetch_california_housing)\n",
    "2. Обернуть его в Dataloader\n",
    "3. Написать архитектуру сети, которая предсказывает стоимость недвижимости. Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
    "4. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели\n",
    "\n",
    "train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25\n",
    "\n",
    "Вопросы? в личку @Kinetikm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaliforniaDataset(torch.utils.data.Dataset):\n",
    "   \n",
    "    def __init__(self, init_dataset, init_target, transform=None):\n",
    "        self._base_dataset = torch.from_numpy(init_dataset).type(torch.float)\n",
    "        self._base_targets = torch.from_numpy(init_target).type(torch.float)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self._base_dataset[idx]\n",
    "        target = self._base_targets[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "      \n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.activation == \"relu\":\n",
    "            return F.relu(x)\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return F.sigmoid(x)\n",
    "        if self.activation == \"leaky_relu\":\n",
    "            return F.leaky_relu(x)\n",
    "        raise RuntimeError\n",
    "        \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = Perceptron(input_dim, hidden_dim, 'leaky_relu')\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dp = nn.Dropout(0.15)\n",
    "        self.fc2 = Perceptron(hidden_dim, 1, 'leaky_relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_california_housing()\n",
    "     \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size = 0.25, random_state = 13)\n",
    "     \n",
    "\n",
    "train_dataset = CaliforniaDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=150, shuffle=False)\n",
    "     \n",
    "\n",
    "test_dataset = CaliforniaDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "     \n",
    "\n",
    "net = FeedForward(8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(opt='adam', num_epochs = 20):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if opt == 'adam':\n",
    "            optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    elif opt == 'rmsprop':\n",
    "            optimizer = optim.RMSprop(net.parameters(), lr=0.001)\n",
    "    elif opt == 'sgd':\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "    for epoch in range(num_epochs):  \n",
    "        running_loss, running_items, r2 = 0.0, 0.0, 0.0\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            fets, target = data[0], data[1]\n",
    "\n",
    "            # обнуляем градиент\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(fets)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_items += len(target)\n",
    "\n",
    "            predict = outputs.data.numpy()\n",
    "            tr_target = target.view(target.shape[0], 1).numpy()\n",
    "            r2 += r2_score(tr_target, predict)\n",
    "\n",
    "            if i % 30 == 0:\n",
    "                net.eval()\n",
    "\n",
    "                data = list(test_loader)[0]\n",
    "\n",
    "                test_outputs = net(data[0])\n",
    "                test_predict = test_outputs.data.numpy()\n",
    "                te_target = data[1].view(data[1].shape[0], 1)\n",
    "                test_r2 = r2_score(te_target, test_predict)\n",
    "\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                      f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                      f'Loss: {running_loss / running_items:.3f}. ' \\\n",
    "                      f'r2: {r2:.3f}. ' \\\n",
    "                      f'Test r2: {test_r2:.3f}')\n",
    "\n",
    "                running_loss, running_items, r2 = 0.0, 0.0, 0.0\n",
    "\n",
    "                net.train()\n",
    "\n",
    "    print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Step [1/104]. Loss: 0.035. r2: -2.551. Test r2: -5.062\n",
      "Epoch [1/50]. Step [31/104]. Loss: 0.029. r2: -72.272. Test r2: -3.544\n",
      "Epoch [1/50]. Step [61/104]. Loss: 0.026. r2: -58.515. Test r2: -3.435\n",
      "Epoch [1/50]. Step [91/104]. Loss: 0.024. r2: -52.224. Test r2: -3.247\n",
      "Epoch [2/50]. Step [1/104]. Loss: 0.025. r2: -1.466. Test r2: -3.176\n",
      "Epoch [2/50]. Step [31/104]. Loss: 0.022. r2: -44.896. Test r2: -2.845\n",
      "Epoch [2/50]. Step [61/104]. Loss: 0.021. r2: -38.019. Test r2: -2.787\n",
      "Epoch [2/50]. Step [91/104]. Loss: 0.019. r2: -33.715. Test r2: -2.622\n",
      "Epoch [3/50]. Step [1/104]. Loss: 0.019. r2: -0.866. Test r2: -2.218\n",
      "Epoch [3/50]. Step [31/104]. Loss: 0.017. r2: -28.856. Test r2: -1.846\n",
      "Epoch [3/50]. Step [61/104]. Loss: 0.016. r2: -22.840. Test r2: -1.038\n",
      "Epoch [3/50]. Step [91/104]. Loss: 0.014. r2: -18.688. Test r2: -1.523\n",
      "Epoch [4/50]. Step [1/104]. Loss: 0.015. r2: -0.441. Test r2: -0.869\n",
      "Epoch [4/50]. Step [31/104]. Loss: 0.013. r2: -14.623. Test r2: -1.213\n",
      "Epoch [4/50]. Step [61/104]. Loss: 0.012. r2: -10.707. Test r2: -0.546\n",
      "Epoch [4/50]. Step [91/104]. Loss: 0.012. r2: -10.958. Test r2: -1.326\n",
      "Epoch [5/50]. Step [1/104]. Loss: 0.012. r2: -0.194. Test r2: -0.929\n",
      "Epoch [5/50]. Step [31/104]. Loss: 0.012. r2: -10.573. Test r2: -1.140\n",
      "Epoch [5/50]. Step [61/104]. Loss: 0.012. r2: -8.689. Test r2: -0.729\n",
      "Epoch [5/50]. Step [91/104]. Loss: 0.011. r2: -7.743. Test r2: -0.220\n",
      "Epoch [6/50]. Step [1/104]. Loss: 0.012. r2: -0.230. Test r2: -0.406\n",
      "Epoch [6/50]. Step [31/104]. Loss: 0.011. r2: -7.615. Test r2: -0.173\n",
      "Epoch [6/50]. Step [61/104]. Loss: 0.011. r2: -6.181. Test r2: -0.108\n",
      "Epoch [6/50]. Step [91/104]. Loss: 0.010. r2: -6.057. Test r2: -0.290\n",
      "Epoch [7/50]. Step [1/104]. Loss: 0.012. r2: -0.178. Test r2: -0.301\n",
      "Epoch [7/50]. Step [31/104]. Loss: 0.010. r2: -5.844. Test r2: -0.201\n",
      "Epoch [7/50]. Step [61/104]. Loss: 0.010. r2: -4.440. Test r2: -0.224\n",
      "Epoch [7/50]. Step [91/104]. Loss: 0.010. r2: -3.879. Test r2: -0.107\n",
      "Epoch [8/50]. Step [1/104]. Loss: 0.011. r2: -0.047. Test r2: -0.201\n",
      "Epoch [8/50]. Step [31/104]. Loss: 0.010. r2: -3.808. Test r2: -0.273\n",
      "Epoch [8/50]. Step [61/104]. Loss: 0.010. r2: -3.667. Test r2: -0.213\n",
      "Epoch [8/50]. Step [91/104]. Loss: 0.010. r2: -3.770. Test r2: -0.195\n",
      "Epoch [9/50]. Step [1/104]. Loss: 0.011. r2: -0.118. Test r2: -0.222\n",
      "Epoch [9/50]. Step [31/104]. Loss: 0.009. r2: -2.877. Test r2: -0.236\n",
      "Epoch [9/50]. Step [61/104]. Loss: 0.010. r2: -2.723. Test r2: -0.154\n",
      "Epoch [9/50]. Step [91/104]. Loss: 0.010. r2: -3.214. Test r2: -0.190\n",
      "Epoch [10/50]. Step [1/104]. Loss: 0.011. r2: -0.082. Test r2: -0.240\n",
      "Epoch [10/50]. Step [31/104]. Loss: 0.009. r2: -2.599. Test r2: -0.241\n",
      "Epoch [10/50]. Step [61/104]. Loss: 0.010. r2: -2.446. Test r2: -0.238\n",
      "Epoch [10/50]. Step [91/104]. Loss: 0.009. r2: -2.979. Test r2: -0.176\n",
      "Epoch [11/50]. Step [1/104]. Loss: 0.011. r2: -0.023. Test r2: -0.223\n",
      "Epoch [11/50]. Step [31/104]. Loss: 0.009. r2: -2.509. Test r2: -0.206\n",
      "Epoch [11/50]. Step [61/104]. Loss: 0.010. r2: -1.711. Test r2: -0.214\n",
      "Epoch [11/50]. Step [91/104]. Loss: 0.009. r2: -2.612. Test r2: -0.207\n",
      "Epoch [12/50]. Step [1/104]. Loss: 0.011. r2: -0.054. Test r2: -0.231\n",
      "Epoch [12/50]. Step [31/104]. Loss: 0.009. r2: -2.716. Test r2: -0.216\n",
      "Epoch [12/50]. Step [61/104]. Loss: 0.010. r2: -1.941. Test r2: -0.227\n",
      "Epoch [12/50]. Step [91/104]. Loss: 0.009. r2: -2.118. Test r2: -0.208\n",
      "Epoch [13/50]. Step [1/104]. Loss: 0.010. r2: -0.101. Test r2: -0.199\n",
      "Epoch [13/50]. Step [31/104]. Loss: 0.009. r2: -2.121. Test r2: -0.238\n",
      "Epoch [13/50]. Step [61/104]. Loss: 0.009. r2: -1.969. Test r2: -0.243\n",
      "Epoch [13/50]. Step [91/104]. Loss: 0.009. r2: -1.010. Test r2: -0.232\n",
      "Epoch [14/50]. Step [1/104]. Loss: 0.011. r2: -0.055. Test r2: -0.221\n",
      "Epoch [14/50]. Step [31/104]. Loss: 0.009. r2: -1.879. Test r2: -0.207\n",
      "Epoch [14/50]. Step [61/104]. Loss: 0.009. r2: -1.731. Test r2: -0.205\n",
      "Epoch [14/50]. Step [91/104]. Loss: 0.009. r2: -1.475. Test r2: -0.208\n",
      "Epoch [15/50]. Step [1/104]. Loss: 0.010. r2: -0.009. Test r2: -0.237\n",
      "Epoch [15/50]. Step [31/104]. Loss: 0.009. r2: -1.142. Test r2: -0.233\n",
      "Epoch [15/50]. Step [61/104]. Loss: 0.009. r2: -1.512. Test r2: -0.215\n",
      "Epoch [15/50]. Step [91/104]. Loss: 0.009. r2: -1.343. Test r2: -0.186\n",
      "Epoch [16/50]. Step [1/104]. Loss: 0.010. r2: -0.083. Test r2: -0.229\n",
      "Epoch [16/50]. Step [31/104]. Loss: 0.009. r2: -1.077. Test r2: -0.226\n",
      "Epoch [16/50]. Step [61/104]. Loss: 0.009. r2: -1.297. Test r2: -0.225\n",
      "Epoch [16/50]. Step [91/104]. Loss: 0.009. r2: -1.558. Test r2: -0.192\n",
      "Epoch [17/50]. Step [1/104]. Loss: 0.010. r2: -0.016. Test r2: -0.227\n",
      "Epoch [17/50]. Step [31/104]. Loss: 0.009. r2: -1.179. Test r2: -0.241\n",
      "Epoch [17/50]. Step [61/104]. Loss: 0.009. r2: -1.324. Test r2: -0.209\n",
      "Epoch [17/50]. Step [91/104]. Loss: 0.009. r2: -1.078. Test r2: -0.193\n",
      "Epoch [18/50]. Step [1/104]. Loss: 0.010. r2: -0.016. Test r2: -0.236\n",
      "Epoch [18/50]. Step [31/104]. Loss: 0.009. r2: -1.090. Test r2: -0.229\n",
      "Epoch [18/50]. Step [61/104]. Loss: 0.009. r2: -0.931. Test r2: -0.235\n",
      "Epoch [18/50]. Step [91/104]. Loss: 0.009. r2: -1.067. Test r2: -0.190\n",
      "Epoch [19/50]. Step [1/104]. Loss: 0.010. r2: -0.012. Test r2: -0.237\n",
      "Epoch [19/50]. Step [31/104]. Loss: 0.009. r2: -0.722. Test r2: -0.233\n",
      "Epoch [19/50]. Step [61/104]. Loss: 0.009. r2: -0.716. Test r2: -0.211\n",
      "Epoch [19/50]. Step [91/104]. Loss: 0.009. r2: -0.730. Test r2: -0.202\n",
      "Epoch [20/50]. Step [1/104]. Loss: 0.010. r2: -0.032. Test r2: -0.238\n",
      "Epoch [20/50]. Step [31/104]. Loss: 0.009. r2: -0.795. Test r2: -0.221\n",
      "Epoch [20/50]. Step [61/104]. Loss: 0.009. r2: -0.598. Test r2: -0.233\n",
      "Epoch [20/50]. Step [91/104]. Loss: 0.009. r2: -0.931. Test r2: -0.187\n",
      "Epoch [21/50]. Step [1/104]. Loss: 0.010. r2: 0.009. Test r2: -0.255\n",
      "Epoch [21/50]. Step [31/104]. Loss: 0.009. r2: -0.568. Test r2: -0.217\n",
      "Epoch [21/50]. Step [61/104]. Loss: 0.009. r2: -0.700. Test r2: -0.215\n",
      "Epoch [21/50]. Step [91/104]. Loss: 0.009. r2: -0.557. Test r2: -0.206\n",
      "Epoch [22/50]. Step [1/104]. Loss: 0.010. r2: 0.002. Test r2: -0.250\n",
      "Epoch [22/50]. Step [31/104]. Loss: 0.009. r2: -0.803. Test r2: -0.234\n",
      "Epoch [22/50]. Step [61/104]. Loss: 0.009. r2: -0.754. Test r2: -0.224\n",
      "Epoch [22/50]. Step [91/104]. Loss: 0.009. r2: -0.585. Test r2: -0.200\n",
      "Epoch [23/50]. Step [1/104]. Loss: 0.010. r2: -0.042. Test r2: -0.248\n",
      "Epoch [23/50]. Step [31/104]. Loss: 0.009. r2: -0.590. Test r2: -0.229\n",
      "Epoch [23/50]. Step [61/104]. Loss: 0.009. r2: -0.850. Test r2: -0.220\n",
      "Epoch [23/50]. Step [91/104]. Loss: 0.009. r2: -0.711. Test r2: -0.210\n",
      "Epoch [24/50]. Step [1/104]. Loss: 0.010. r2: -0.035. Test r2: -0.245\n",
      "Epoch [24/50]. Step [31/104]. Loss: 0.009. r2: -0.580. Test r2: -0.225\n",
      "Epoch [24/50]. Step [61/104]. Loss: 0.009. r2: -0.637. Test r2: -0.221\n",
      "Epoch [24/50]. Step [91/104]. Loss: 0.009. r2: -0.434. Test r2: -0.199\n",
      "Epoch [25/50]. Step [1/104]. Loss: 0.010. r2: -0.040. Test r2: -0.247\n",
      "Epoch [25/50]. Step [31/104]. Loss: 0.009. r2: -0.456. Test r2: -0.224\n",
      "Epoch [25/50]. Step [61/104]. Loss: 0.009. r2: -0.374. Test r2: -0.227\n",
      "Epoch [25/50]. Step [91/104]. Loss: 0.009. r2: -0.301. Test r2: -0.198\n",
      "Epoch [26/50]. Step [1/104]. Loss: 0.010. r2: 0.010. Test r2: -0.248\n",
      "Epoch [26/50]. Step [31/104]. Loss: 0.009. r2: -0.525. Test r2: -0.224\n",
      "Epoch [26/50]. Step [61/104]. Loss: 0.009. r2: -0.452. Test r2: -0.228\n",
      "Epoch [26/50]. Step [91/104]. Loss: 0.009. r2: -0.560. Test r2: -0.198\n",
      "Epoch [27/50]. Step [1/104]. Loss: 0.010. r2: 0.003. Test r2: -0.249\n",
      "Epoch [27/50]. Step [31/104]. Loss: 0.009. r2: -0.378. Test r2: -0.223\n",
      "Epoch [27/50]. Step [61/104]. Loss: 0.009. r2: -0.335. Test r2: -0.224\n",
      "Epoch [27/50]. Step [91/104]. Loss: 0.009. r2: -0.352. Test r2: -0.201\n",
      "Epoch [28/50]. Step [1/104]. Loss: 0.010. r2: -0.027. Test r2: -0.252\n",
      "Epoch [28/50]. Step [31/104]. Loss: 0.009. r2: -0.326. Test r2: -0.224\n",
      "Epoch [28/50]. Step [61/104]. Loss: 0.009. r2: -0.402. Test r2: -0.223\n",
      "Epoch [28/50]. Step [91/104]. Loss: 0.009. r2: -0.208. Test r2: -0.202\n",
      "Epoch [29/50]. Step [1/104]. Loss: 0.010. r2: -0.006. Test r2: -0.249\n",
      "Epoch [29/50]. Step [31/104]. Loss: 0.009. r2: -0.278. Test r2: -0.226\n",
      "Epoch [29/50]. Step [61/104]. Loss: 0.009. r2: -0.256. Test r2: -0.224\n",
      "Epoch [29/50]. Step [91/104]. Loss: 0.009. r2: -0.236. Test r2: -0.208\n",
      "Epoch [30/50]. Step [1/104]. Loss: 0.010. r2: 0.001. Test r2: -0.256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50]. Step [31/104]. Loss: 0.009. r2: -0.334. Test r2: -0.228\n",
      "Epoch [30/50]. Step [61/104]. Loss: 0.009. r2: -0.392. Test r2: -0.228\n",
      "Epoch [30/50]. Step [91/104]. Loss: 0.009. r2: -0.268. Test r2: -0.204\n",
      "Epoch [31/50]. Step [1/104]. Loss: 0.010. r2: 0.004. Test r2: -0.258\n",
      "Epoch [31/50]. Step [31/104]. Loss: 0.009. r2: -0.281. Test r2: -0.229\n",
      "Epoch [31/50]. Step [61/104]. Loss: 0.009. r2: -0.375. Test r2: -0.226\n",
      "Epoch [31/50]. Step [91/104]. Loss: 0.009. r2: -0.246. Test r2: -0.206\n",
      "Epoch [32/50]. Step [1/104]. Loss: 0.010. r2: -0.019. Test r2: -0.264\n",
      "Epoch [32/50]. Step [31/104]. Loss: 0.009. r2: -0.224. Test r2: -0.230\n",
      "Epoch [32/50]. Step [61/104]. Loss: 0.009. r2: -0.267. Test r2: -0.232\n",
      "Epoch [32/50]. Step [91/104]. Loss: 0.009. r2: -0.233. Test r2: -0.203\n",
      "Epoch [33/50]. Step [1/104]. Loss: 0.010. r2: -0.008. Test r2: -0.260\n",
      "Epoch [33/50]. Step [31/104]. Loss: 0.009. r2: -0.286. Test r2: -0.231\n",
      "Epoch [33/50]. Step [61/104]. Loss: 0.009. r2: -0.293. Test r2: -0.233\n",
      "Epoch [33/50]. Step [91/104]. Loss: 0.009. r2: -0.252. Test r2: -0.208\n",
      "Epoch [34/50]. Step [1/104]. Loss: 0.010. r2: 0.003. Test r2: -0.264\n",
      "Epoch [34/50]. Step [31/104]. Loss: 0.009. r2: -0.257. Test r2: -0.233\n",
      "Epoch [34/50]. Step [61/104]. Loss: 0.009. r2: -0.256. Test r2: -0.236\n",
      "Epoch [34/50]. Step [91/104]. Loss: 0.009. r2: -0.237. Test r2: -0.207\n",
      "Epoch [35/50]. Step [1/104]. Loss: 0.010. r2: -0.012. Test r2: -0.265\n",
      "Epoch [35/50]. Step [31/104]. Loss: 0.009. r2: -0.263. Test r2: -0.231\n",
      "Epoch [35/50]. Step [61/104]. Loss: 0.009. r2: -0.251. Test r2: -0.233\n",
      "Epoch [35/50]. Step [91/104]. Loss: 0.009. r2: -0.219. Test r2: -0.206\n",
      "Epoch [36/50]. Step [1/104]. Loss: 0.010. r2: -0.000. Test r2: -0.264\n",
      "Epoch [36/50]. Step [31/104]. Loss: 0.009. r2: -0.251. Test r2: -0.231\n",
      "Epoch [36/50]. Step [61/104]. Loss: 0.009. r2: -0.306. Test r2: -0.229\n",
      "Epoch [36/50]. Step [91/104]. Loss: 0.009. r2: -0.226. Test r2: -0.203\n",
      "Epoch [37/50]. Step [1/104]. Loss: 0.010. r2: -0.000. Test r2: -0.263\n",
      "Epoch [37/50]. Step [31/104]. Loss: 0.009. r2: -0.237. Test r2: -0.230\n",
      "Epoch [37/50]. Step [61/104]. Loss: 0.009. r2: -0.256. Test r2: -0.236\n",
      "Epoch [37/50]. Step [91/104]. Loss: 0.009. r2: -0.154. Test r2: -0.209\n",
      "Epoch [38/50]. Step [1/104]. Loss: 0.010. r2: -0.006. Test r2: -0.267\n",
      "Epoch [38/50]. Step [31/104]. Loss: 0.009. r2: -0.219. Test r2: -0.228\n",
      "Epoch [38/50]. Step [61/104]. Loss: 0.009. r2: -0.212. Test r2: -0.233\n",
      "Epoch [38/50]. Step [91/104]. Loss: 0.009. r2: -0.206. Test r2: -0.203\n",
      "Epoch [39/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.266\n",
      "Epoch [39/50]. Step [31/104]. Loss: 0.009. r2: -0.188. Test r2: -0.230\n",
      "Epoch [39/50]. Step [61/104]. Loss: 0.009. r2: -0.197. Test r2: -0.235\n",
      "Epoch [39/50]. Step [91/104]. Loss: 0.009. r2: -0.196. Test r2: -0.208\n",
      "Epoch [40/50]. Step [1/104]. Loss: 0.010. r2: 0.001. Test r2: -0.266\n",
      "Epoch [40/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.227\n",
      "Epoch [40/50]. Step [61/104]. Loss: 0.009. r2: -0.245. Test r2: -0.236\n",
      "Epoch [40/50]. Step [91/104]. Loss: 0.009. r2: -0.176. Test r2: -0.211\n",
      "Epoch [41/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.270\n",
      "Epoch [41/50]. Step [31/104]. Loss: 0.009. r2: -0.198. Test r2: -0.229\n",
      "Epoch [41/50]. Step [61/104]. Loss: 0.009. r2: -0.189. Test r2: -0.235\n",
      "Epoch [41/50]. Step [91/104]. Loss: 0.009. r2: -0.174. Test r2: -0.209\n",
      "Epoch [42/50]. Step [1/104]. Loss: 0.010. r2: 0.003. Test r2: -0.269\n",
      "Epoch [42/50]. Step [31/104]. Loss: 0.009. r2: -0.179. Test r2: -0.230\n",
      "Epoch [42/50]. Step [61/104]. Loss: 0.009. r2: -0.229. Test r2: -0.235\n",
      "Epoch [42/50]. Step [91/104]. Loss: 0.009. r2: -0.158. Test r2: -0.211\n",
      "Epoch [43/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.270\n",
      "Epoch [43/50]. Step [31/104]. Loss: 0.009. r2: -0.182. Test r2: -0.229\n",
      "Epoch [43/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.236\n",
      "Epoch [43/50]. Step [91/104]. Loss: 0.009. r2: -0.145. Test r2: -0.210\n",
      "Epoch [44/50]. Step [1/104]. Loss: 0.010. r2: -0.003. Test r2: -0.270\n",
      "Epoch [44/50]. Step [31/104]. Loss: 0.009. r2: -0.168. Test r2: -0.227\n",
      "Epoch [44/50]. Step [61/104]. Loss: 0.009. r2: -0.226. Test r2: -0.235\n",
      "Epoch [44/50]. Step [91/104]. Loss: 0.009. r2: -0.158. Test r2: -0.208\n",
      "Epoch [45/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.270\n",
      "Epoch [45/50]. Step [31/104]. Loss: 0.009. r2: -0.174. Test r2: -0.228\n",
      "Epoch [45/50]. Step [61/104]. Loss: 0.009. r2: -0.224. Test r2: -0.236\n",
      "Epoch [45/50]. Step [91/104]. Loss: 0.009. r2: -0.167. Test r2: -0.207\n",
      "Epoch [46/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.268\n",
      "Epoch [46/50]. Step [31/104]. Loss: 0.009. r2: -0.190. Test r2: -0.228\n",
      "Epoch [46/50]. Step [61/104]. Loss: 0.009. r2: -0.229. Test r2: -0.235\n",
      "Epoch [46/50]. Step [91/104]. Loss: 0.009. r2: -0.179. Test r2: -0.207\n",
      "Epoch [47/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.270\n",
      "Epoch [47/50]. Step [31/104]. Loss: 0.009. r2: -0.167. Test r2: -0.228\n",
      "Epoch [47/50]. Step [61/104]. Loss: 0.009. r2: -0.220. Test r2: -0.235\n",
      "Epoch [47/50]. Step [91/104]. Loss: 0.009. r2: -0.134. Test r2: -0.209\n",
      "Epoch [48/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.271\n",
      "Epoch [48/50]. Step [31/104]. Loss: 0.009. r2: -0.160. Test r2: -0.228\n",
      "Epoch [48/50]. Step [61/104]. Loss: 0.009. r2: -0.208. Test r2: -0.238\n",
      "Epoch [48/50]. Step [91/104]. Loss: 0.009. r2: -0.156. Test r2: -0.211\n",
      "Epoch [49/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.272\n",
      "Epoch [49/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.230\n",
      "Epoch [49/50]. Step [61/104]. Loss: 0.009. r2: -0.204. Test r2: -0.239\n",
      "Epoch [49/50]. Step [91/104]. Loss: 0.009. r2: -0.163. Test r2: -0.209\n",
      "Epoch [50/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.271\n",
      "Epoch [50/50]. Step [31/104]. Loss: 0.009. r2: -0.194. Test r2: -0.228\n",
      "Epoch [50/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.236\n",
      "Epoch [50/50]. Step [91/104]. Loss: 0.009. r2: -0.152. Test r2: -0.208\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(opt='adam', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.399\n",
      "Epoch [1/50]. Step [31/104]. Loss: 0.009. r2: -0.196. Test r2: -0.204\n",
      "Epoch [1/50]. Step [61/104]. Loss: 0.009. r2: -0.212. Test r2: -0.218\n",
      "Epoch [1/50]. Step [91/104]. Loss: 0.009. r2: -0.154. Test r2: -0.238\n",
      "Epoch [2/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.302\n",
      "Epoch [2/50]. Step [31/104]. Loss: 0.009. r2: -0.177. Test r2: -0.213\n",
      "Epoch [2/50]. Step [61/104]. Loss: 0.009. r2: -0.204. Test r2: -0.222\n",
      "Epoch [2/50]. Step [91/104]. Loss: 0.009. r2: -0.146. Test r2: -0.233\n",
      "Epoch [3/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.296\n",
      "Epoch [3/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.215\n",
      "Epoch [3/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.223\n",
      "Epoch [3/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [4/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.294\n",
      "Epoch [4/50]. Step [31/104]. Loss: 0.009. r2: -0.177. Test r2: -0.216\n",
      "Epoch [4/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.223\n",
      "Epoch [4/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [5/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [5/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.217\n",
      "Epoch [5/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.224\n",
      "Epoch [5/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.233\n",
      "Epoch [6/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.294\n",
      "Epoch [6/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [6/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [6/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [7/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [7/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [7/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.223\n",
      "Epoch [7/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [8/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [8/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [8/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.223\n",
      "Epoch [8/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [9/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [9/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [9/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.223\n",
      "Epoch [9/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [10/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [10/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [10/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [10/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [11/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [11/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [11/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [11/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [12/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [12/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [12/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [12/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [13/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [13/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [13/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [13/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [14/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [14/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [14/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [14/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [15/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [15/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [15/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.223\n",
      "Epoch [15/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [16/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [16/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [16/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.224\n",
      "Epoch [16/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [17/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [17/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [17/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.223\n",
      "Epoch [17/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [18/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [18/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [18/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [18/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [19/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [19/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [19/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [19/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [20/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [20/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [20/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.224\n",
      "Epoch [20/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [21/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [21/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [21/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.223\n",
      "Epoch [21/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [22/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [22/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [22/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.224\n",
      "Epoch [22/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [23/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [23/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [23/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.223\n",
      "Epoch [23/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [24/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [24/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [24/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.223\n",
      "Epoch [24/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [25/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [25/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [25/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [25/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [26/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [26/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [26/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [26/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [27/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [27/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [27/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [27/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [28/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.293\n",
      "Epoch [28/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [28/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [28/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [29/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [29/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [29/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [29/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [30/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [30/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.224\n",
      "Epoch [30/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [31/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.293\n",
      "Epoch [31/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [31/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [31/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [32/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [32/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [32/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.223\n",
      "Epoch [32/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [33/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [33/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [33/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.224\n",
      "Epoch [33/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [34/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [34/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [34/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [34/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [35/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [35/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [35/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [35/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [36/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [36/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [36/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.223\n",
      "Epoch [36/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [37/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [37/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [37/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [37/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [38/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [38/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [38/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [38/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [39/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [39/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [39/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [39/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [40/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.292\n",
      "Epoch [40/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [40/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.224\n",
      "Epoch [40/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [41/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [41/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [41/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [41/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [42/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [42/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [42/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [42/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [43/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.292\n",
      "Epoch [43/50]. Step [31/104]. Loss: 0.009. r2: -0.174. Test r2: -0.216\n",
      "Epoch [43/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [43/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [44/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [44/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [44/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.223\n",
      "Epoch [44/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [45/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [45/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [45/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [45/50]. Step [91/104]. Loss: 0.009. r2: -0.141. Test r2: -0.232\n",
      "Epoch [46/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [46/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [46/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [46/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [47/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [47/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [47/50]. Step [61/104]. Loss: 0.009. r2: -0.207. Test r2: -0.224\n",
      "Epoch [47/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [48/50]. Step [1/104]. Loss: 0.010. r2: -0.002. Test r2: -0.293\n",
      "Epoch [48/50]. Step [31/104]. Loss: 0.009. r2: -0.175. Test r2: -0.216\n",
      "Epoch [48/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [48/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.232\n",
      "Epoch [49/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [49/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [49/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [49/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Epoch [50/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.293\n",
      "Epoch [50/50]. Step [31/104]. Loss: 0.009. r2: -0.176. Test r2: -0.216\n",
      "Epoch [50/50]. Step [61/104]. Loss: 0.009. r2: -0.206. Test r2: -0.224\n",
      "Epoch [50/50]. Step [91/104]. Loss: 0.009. r2: -0.143. Test r2: -0.232\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(opt='rmsprop', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Step [1/104]. Loss: 0.010. r2: -0.001. Test r2: -0.288\n",
      "Epoch [1/50]. Step [31/104]. Loss: 0.009. r2: -0.202. Test r2: -0.268\n",
      "Epoch [1/50]. Step [61/104]. Loss: 0.009. r2: -0.210. Test r2: -0.258\n",
      "Epoch [1/50]. Step [91/104]. Loss: 0.009. r2: -0.159. Test r2: -0.246\n",
      "Epoch [2/50]. Step [1/104]. Loss: 0.010. r2: -0.003. Test r2: -0.255\n",
      "Epoch [2/50]. Step [31/104]. Loss: 0.009. r2: -0.163. Test r2: -0.245\n",
      "Epoch [2/50]. Step [61/104]. Loss: 0.009. r2: -0.196. Test r2: -0.241\n",
      "Epoch [2/50]. Step [91/104]. Loss: 0.009. r2: -0.145. Test r2: -0.234\n",
      "Epoch [3/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.245\n",
      "Epoch [3/50]. Step [31/104]. Loss: 0.009. r2: -0.154. Test r2: -0.237\n",
      "Epoch [3/50]. Step [61/104]. Loss: 0.009. r2: -0.193. Test r2: -0.236\n",
      "Epoch [3/50]. Step [91/104]. Loss: 0.009. r2: -0.142. Test r2: -0.230\n",
      "Epoch [4/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.241\n",
      "Epoch [4/50]. Step [31/104]. Loss: 0.009. r2: -0.152. Test r2: -0.235\n",
      "Epoch [4/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.234\n",
      "Epoch [4/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [5/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.240\n",
      "Epoch [5/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.234\n",
      "Epoch [5/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [5/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [6/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.240\n",
      "Epoch [6/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.234\n",
      "Epoch [6/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [6/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [7/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.240\n",
      "Epoch [7/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.234\n",
      "Epoch [7/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [7/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [8/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [8/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [8/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [8/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [9/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [9/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [9/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [9/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [10/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [10/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [10/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [10/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [11/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [11/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [11/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [11/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [12/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [12/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [12/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [12/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [13/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [13/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [13/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [13/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [14/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [14/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [14/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [14/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [15/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [15/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [15/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [15/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [16/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [16/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [16/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [16/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [17/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [17/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [17/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [17/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [18/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [18/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [18/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [18/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [19/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [19/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [19/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [19/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [20/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [20/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [20/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [20/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [21/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [21/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [21/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [21/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [22/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [22/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [22/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [22/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [23/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [23/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [23/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [23/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [24/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [24/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [24/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [24/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [25/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [25/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [25/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [25/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [26/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [26/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [26/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [26/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [27/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [27/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [27/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [27/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [28/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [28/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [28/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [28/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [29/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [29/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [29/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [29/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [30/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [30/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [30/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [31/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [31/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [31/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [31/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [32/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [32/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [32/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [32/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [33/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [33/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [33/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [33/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [34/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [34/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [34/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [34/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [35/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [35/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [35/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [35/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [36/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [36/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [36/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [36/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [37/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [37/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [37/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [37/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [38/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [38/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [38/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [38/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [39/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [39/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [39/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [39/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [40/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [40/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [40/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [40/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [41/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [41/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [41/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [41/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [42/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [42/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [42/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [42/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [43/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [43/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [43/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [43/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [44/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [44/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [44/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [44/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [45/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [45/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [45/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [45/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [46/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [46/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [46/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [46/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [47/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [47/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [47/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [47/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [48/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [48/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [48/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [48/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [49/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [49/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [49/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [49/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Epoch [50/50]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.239\n",
      "Epoch [50/50]. Step [31/104]. Loss: 0.009. r2: -0.151. Test r2: -0.233\n",
      "Epoch [50/50]. Step [61/104]. Loss: 0.009. r2: -0.192. Test r2: -0.233\n",
      "Epoch [50/50]. Step [91/104]. Loss: 0.009. r2: -0.140. Test r2: -0.229\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(opt='sgd', num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель с оптимизатором Adam показала лучшие результаты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
